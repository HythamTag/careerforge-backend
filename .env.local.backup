# =========================================================
# CV ENHANCER — ENVIRONMENT CONFIGURATION
# =========================================================
# Copy to .env
# NEVER commit real secrets
# =========================================================

# =========================================================
# APPLICATION
# =========================================================

NODE_ENV=development
PORT=5000
HOST=localhost

# =========================================================
# DATABASE (MongoDB)
# =========================================================
# Local development (outside Docker):
# mongodb://admin:password@localhost:27017/...

# Docker Compose:
# mongodb://admin:password@mongodb:27017/...

MONGODB_URI=mongodb://127.0.0.1:27017/cv_enhancer
DB_NAME=cv_enhancer
DATABASE_TIMEOUT=10000

# =========================================================
# REDIS (CACHE + JOB QUEUE)
# =========================================================

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# =========================================================
# AI PROVIDER SELECTION
# =========================================================
# openai | anthropic | gemini | huggingface | ollama | mock

AI_PROVIDER=ollama

# =========================================================
# AI MODELS — PARSER (CV → TEXT)
# =========================================================
# Model selection per provider - hardware tuning is in docker-compose.yml

AI_MODEL_PARSER_OPENAI=gpt-4o-mini
AI_MODEL_PARSER_ANTHROPIC=claude-3-haiku-20240307
AI_MODEL_PARSER_GEMINI=gemini-2.0-flash
AI_MODEL_PARSER_HUGGINGFACE=google/gemma-2-2b-it
AI_MODEL_PARSER_OLLAMA=gemma2:2b
AI_MODEL_ATS=gemma2:2b

# =========================================================
# AI MODELS — OPTIMIZER (CV ENHANCEMENT)
# =========================================================

AI_MODEL_OPTIMIZER_OPENAI=gpt-4o-mini
AI_MODEL_OPTIMIZER_ANTHROPIC=claude-3-haiku-20240307
AI_MODEL_OPTIMIZER_GEMINI=gemini-2.0-flash
AI_MODEL_OPTIMIZER_HUGGINGFACE=google/gemma-2-2b-it
AI_MODEL_OPTIMIZER_OLLAMA=gemma2:9b

# =========================================================
# AI MODELS — ATS FEEDBACK
# =========================================================

AI_MODEL_ATS_FEEDBACK=phi3:mini

# =========================================================
# API KEYS (ONLY FOR CLOUD PROVIDERS)
# =========================================================

OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GEMINI_API_KEY=your-gemini-key
HUGGINGFACE_API_KEY=your-huggingface-key

# =========================================================
# OLLAMA (LOCAL AI) — ROUTING ONLY
# =========================================================
# IMPORTANT: All hardware tuning (CTX, Batch, Threads, GPU)
# is managed EXCLUSIVELY in docker-compose.yml via profiles.
# This file contains ONLY logical routing configuration.
# =========================================================

# Default host (for legacy compatibility)
OLLAMA_HOST=http://localhost:11434

# Task-specific Ollama endpoints
# These route to isolated containers with different hardware tuning
OLLAMA_PARSER_HOST=http://localhost:11434
OLLAMA_OPTIMIZER_HOST=http://localhost:11435
OLLAMA_ATS_HOST=http://localhost:11436

# Connection timeout (ms) - 5 minutes for large parsing jobs
OLLAMA_TIMEOUT=600000

# =========================================================
# PDF GENERATION (PUPPETEER)
# =========================================================

PUPPETEER_WS_ENDPOINT=ws://localhost:3000

# =========================================================
# STORAGE
# =========================================================

STORAGE_TYPE=local
STORAGE_LOCAL_PATH=./uploads

# --- AWS S3 (optional) ---
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1
S3_BUCKET_NAME=
S3_PUBLIC_READ=false

# =========================================================
# FILE LIMITS
# =========================================================

MAX_FILE_SIZE=10485760
MAX_PAGES=8
ALLOWED_MIME_TYPES=application/pdf

# =========================================================
# RATE LIMITING
# =========================================================

RATE_LIMIT_UPLOADS=10
RATE_LIMIT_WINDOW_MS=3600000
RATE_LIMIT_MAX_REQUESTS=100

# =========================================================
# SECURITY
# =========================================================

CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:8080

JWT_SECRET=change-this-super-secret-key
JWT_EXPIRES_IN=24h

# =========================================================
# LOGGING
# =========================================================

LOG_LEVEL=info
LOG_MAX_SIZE=20m
LOG_MAX_FILES=5

# =========================================================
# JOB QUEUE
# =========================================================

JOB_MAX_ATTEMPTS=3
JOB_BACKOFF_DELAY=2000
JOB_REMOVE_ON_COMPLETE_AGE=3600
JOB_REMOVE_ON_COMPLETE_COUNT=100
JOB_REMOVE_ON_FAIL_AGE=86400

# =========================================================
# MONITORING & PERFORMANCE
# =========================================================

ENABLE_METRICS=true
METRICS_PORT=9090
HEALTH_CHECK_INTERVAL=30000

SLOW_REQUEST_THRESHOLD=1000
VERY_SLOW_REQUEST_THRESHOLD=5000

# =========================================================
# TIMEOUTS
# =========================================================

HTTP_TIMEOUT=120000
AI_REQUEST_TIMEOUT=180000

# =========================================================
# JOB QUEUE CONCURRENCY
# =========================================================
JOB_QUEUE_PARSING_CONCURRENCY=5
JOB_QUEUE_PARSING_PRIORITY=10
JOB_QUEUE_ENHANCEMENT_CONCURRENCY=2
JOB_QUEUE_ENHANCEMENT_PRIORITY=5
JOB_QUEUE_GENERATION_CONCURRENCY=2
JOB_QUEUE_GENERATION_PRIORITY=5
JOB_QUEUE_WEBHOOK_DELIVERY_CONCURRENCY=10
JOB_QUEUE_WEBHOOK_DELIVERY_PRIORITY=1

# =========================================================
# JOB QUEUE LIMITS
# =========================================================
JOB_LIMITER_MAX=100
JOB_LIMITER_DURATION=10000
